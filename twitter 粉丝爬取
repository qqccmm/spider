import csv,time
from bs4 import BeautifulSoup as bs
from selenium import webdriver
options = webdriver.ChromeOptions() 
options.add_experimental_option("debuggerAddress", "127.0.0.1:9222")
chromedriver_path = "/usr/local/bin/chromedriver" #驱动路径
# driver = webdriver.Chrome(executable_path=chromedriver_path, options=options)
#先在终端打开： Google\ Chrome --remote-debugging-port=9222 --user-data-dir="~/ChromeProfile"

def get_fans(name,url_name,path):
    driver = webdriver.Chrome(executable_path=chromedriver_path, options=options) 
    url = "https://twitter.com/{0}/followers".format(url_name)
    driver.get(url)
    driver = webdriver.Chrome(executable_path=chromedriver_path, options=options)

    with open( path + '/{0}的粉丝.csv'.format(name),'a') as f: 
        zone = csv.writer(f)
        head = ['粉丝名字','粉丝关注者网页id']
        zone.writerows([head])
    limit = 1000 #想要爬取多少粉丝数量   
    followers_name_data = set()
    url_name_data = set() #利用set不可重复性 
    old_scroll_height = -1
    js1 = 'return document.body.scrollHeight'  # 获取页面高度的javascript语句
    js2 = 'window.scrollTo(0, document.body.scrollHeight)' # 将页面下拉的Javascript语句
    flag = True
    while(flag):  
        old_scroll_height = driver.execute_script(js1) 
        driver.execute_script(js2) # 模拟浏览器进行滚动下拉
        time.sleep(2)
        content = bs(driver.page_source, 'html.parser')  # 解析网页
        names = content.find_all(
            'div', 'css-901oao css-bfa6kz r-18jsvk2 r-1qd0xha r-a023e6 r-b88u0q r-ad9z0x r-bcqeeo r-3s2u2q r-qvutc0')  # 找到所有用户数据
        url_names = content.find_all(
            'div', 'css-901oao css-bfa6kz r-m0bqgq r-18u37iz r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-qvutc0')  # 找到所有用户数据
        for followers_name in names[1:-3]: #运用美味汤的API
            followers_name_data.add(followers_name.string)  
        for url_name in url_names[1:-3]: #删去最开始的一个后最后三个名字
            url_name_data.add(url_name.string)
        if (driver.execute_script(js1) > old_scroll_height): #检查是否到底
            pass
        else:
            time.sleep(3) #检查是否到底时常有bug
            driver.execute_script(js2) # 模拟浏览器进行滚动下拉
            time.sleep(3) 
            if (driver.execute_script(js1) == old_scroll_height):
                flag = False
        if len(followers_name_data) >= limit:  # 判断是否达到指定的爬取数量，达到则跳出循环
            flag = False
            
    print("{}名关注者爬取完毕".format(len(followers_name_data)))        
    with open(path + '/{0}的粉丝.csv'.format(name),'a') as f: 
        for followers_name,url_name in  zip(followers_name_data,url_name_data):
            zone = csv.writer(f)
            zone.writerows([[followers_name,url_name]])
    
get_fans(name = 'doublelift',url_name='doublelift1',path='/Users/shanshan/Desktop/python')


